import org.apache.spark.sql.SparkSession

object OncoStreamApp {
  def main(args: Array[String]): Unit = {
    
    // 1. Initialiser le Cerveau (Spark Session)
    val spark = SparkSession.builder()
      .appName("OncoStream-Ingestion")
      // "local[*]" signifie : Utilise tous les c≈ìurs de MON processeur (mode test)
      .master("local[*]")
      .getOrCreate()

    // On r√©duit le bruit (logs) pour ne voir que les erreurs ou les donn√©es
    spark.sparkContext.setLogLevel("WARN")
    
    println("üß¨ D√©marrage du Job Spark OncoStream...")

    // 2. L'Oreille : Configuration de la lecture Kafka
    // Note : On utilise 'localhost:9092' car tu lances ce code depuis WSL (l'ext√©rieur du conteneur)
    val kafkaStream = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "kafka:29092") 
      .option("subscribe", "ngs-raw-reads") // Le topic cr√©√© par ton script Python
      .option("startingOffsets", "latest")    // On √©coute seulement les nouveaux messages
      .load()

    // 3. La Traduction : Kafka envoie des octets (binaire), on veut du texte
    import spark.implicits._
    val dataStream = kafkaStream.selectExpr("CAST(value AS STRING) as fastq_data")

    // 4. L'Affichage : √âcrire le r√©sultat dans la console
    val query = dataStream.writeStream
      .outputMode("append") // On ajoute chaque nouvelle ligne re√ßue
      .format("console")    // Sortie = √âcran noir
      .option("truncate", "false") // Affiche toute la ligne sans couper
      .start()

    // Garde le programme allum√© ind√©finiment
    query.awaitTermination()
  }
}